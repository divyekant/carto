# Carto â€” Environment Variables
# Copy this file to .env and fill in your values.

# LLM Provider: anthropic (default), openai, openrouter, ollama
LLM_PROVIDER=anthropic

# API key for your LLM provider
# Anthropic: sk-ant-api03-... or OAuth token sk-ant-oat01-...
# OpenAI: sk-...
# Takes priority over ANTHROPIC_API_KEY if both are set
LLM_API_KEY=

# Legacy Anthropic key (used if LLM_API_KEY is empty)
ANTHROPIC_API_KEY=

# Base URL override (required for OpenRouter, Ollama, self-hosted)
# OpenAI default: https://api.openai.com
# Ollama default: http://localhost:11434
LLM_BASE_URL=

# Model for fast/cheap analysis (atoms, summaries)
# Anthropic default: claude-haiku-4-5-20251001
# OpenAI example: gpt-4.1-mini
# Ollama example: llama3:8b
CARTO_FAST_MODEL=

# Model for deep/expensive analysis (wiring, zones, blueprint)
# Anthropic default: claude-opus-4-6
# OpenAI example: gpt-4.1
# Ollama example: llama3:70b
CARTO_DEEP_MODEL=

# Maximum concurrent LLM requests (default: 10)
CARTO_MAX_CONCURRENT=10

# Memories server URL
MEMORIES_URL=http://localhost:8900

# Memories server API key
MEMORIES_API_KEY=

# For Docker: directory to mount as /projects (read-only)
PROJECTS_DIR=~/projects
